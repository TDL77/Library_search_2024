{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.39.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "st.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import mmap\n",
    "import pickle\n",
    "from typing import List, Tuple\n",
    "from functools import cache\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from loguru import logger\n",
    "from diskcache import Cache\n",
    "\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers.ensemble import EnsembleRetriever\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import HumanMessage, Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from bert_score import score  # Импортируем BERTScore\n",
    "\n",
    "from colorama import Fore, Back, Style\n",
    "import random\n",
    "\n",
    "import asyncio\n",
    "from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, validator, constr\n",
    "from typing import List, Dict, Optional, Set\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "import aiofiles\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import mimetypes\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from functools import lru_cache\n",
    "import redis\n",
    "import pypdf\n",
    "from pptx import Presentation\n",
    "from docx import Document\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pymorphy2\n",
    "from natasha import (\n",
    "    Segmenter, MorphVocab, NewsEmbedding, \n",
    "    NewsMorphTagger, NewsSyntaxParser, NewsNERTagger, Doc\n",
    ")\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import os\n",
    "import tempfile\n",
    "import aiohttp\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "import os\n",
    "import hashlib\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from functools import lru_cache\n",
    "\n",
    "import pandas as pd\n",
    "import pypdf\n",
    "from docx import Document as DocxDocument\n",
    "from pptx import Presentation\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Настройка логирования\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score  # Импортируем BERTScore\n",
    "\n",
    "# Функция для оценки релевантности на основе BERTScore\n",
    "def evaluate_relevance(query, retrieved_doc, generated_answer):\n",
    "    P, R, F1 = score([generated_answer], [retrieved_doc], lang=\"ru\")\n",
    "    return F1.mean().item()\n",
    "\n",
    "def evaluate_relevance_score(query, doc_text):\n",
    "    \"\"\"Вычисляем BERTScore между запросом и текстом документа\"\"\"\n",
    "    _, _, F1 = score([query], [doc_text], lang=\"ru\")\n",
    "    return F1.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataclass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO)\n\u001b[1;32m     19\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mConfig\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Конфигурация для обработки документов\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     chunk_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataclass' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import mmap\n",
    "import hashlib\n",
    "from typing import List, Set, Dict\n",
    "from functools import lru_cache\n",
    "import logging\n",
    "import json\n",
    "from docx import Document as DocxDocument\n",
    "import pandas as pd\n",
    "from pptx import Presentation\n",
    "import pypdf\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import re\n",
    "\n",
    "# Настройка логирования\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Конфигурация для обработки документов\"\"\"\n",
    "    chunk_size: int = 512\n",
    "    chunk_overlap: int = 200\n",
    "    cache_dir: str = \"./cache\"\n",
    "    supported_extensions: set = frozenset({'.docx', '.xlsx', '.pptx', '.pdf', '.csv', '.md', '.json' , '.txt'})\n",
    "\n",
    "class DocumentLoader:\n",
    "    \"\"\"Document loader with recursive directory traversal and caching\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Приоритетные разделители\n",
    "            chunk_size=config.chunk_size,\n",
    "            chunk_overlap=config.chunk_overlap,\n",
    "            length_function=len,\n",
    "            keep_separator=False\n",
    "        )\n",
    "        self._path_hash_cache = {}\n",
    "        \n",
    "        # Создаем директорию для кэша если её нет\n",
    "        os.makedirs(config.cache_dir, exist_ok=True)\n",
    "        \n",
    "    def _calculate_path_hash(self, path: str) -> str:\n",
    "        \"\"\"Вычисляет хэш структуры директории и содержимого файлов\"\"\"\n",
    "        hasher = hashlib.sha256()\n",
    "        \n",
    "        for root, dirs, files in os.walk(path):\n",
    "            dirs.sort()\n",
    "            files.sort()\n",
    "            \n",
    "            hasher.update(root.encode('utf-8'))\n",
    "            \n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                if Path(file_path).suffix.lower() in self.config.supported_extensions:\n",
    "                    stats = os.stat(file_path)\n",
    "                    file_info = f\"{file_path}|{stats.st_mtime}|{stats.st_size}\"\n",
    "                    hasher.update(file_info.encode('utf-8'))\n",
    "                    \n",
    "        return hasher.hexdigest()\n",
    "\n",
    "    def _get_all_files(self, path: str) -> List[str]:\n",
    "        \"\"\"Рекурсивно получает все поддерживаемые файлы из директории\"\"\"\n",
    "        files = []\n",
    "        for root, _, filenames in os.walk(path):\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(root, filename)\n",
    "                if Path(file_path).suffix.lower() in self.config.supported_extensions:\n",
    "                    files.append(file_path)\n",
    "        return sorted(files)\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Предварительная обработка текста перед разбиением на чанки\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "            \n",
    "        # Заменяем множественные переносы строк на одинарный\n",
    "        text = re.sub(r'\\n\\s*\\n+', '\\n', text)\n",
    "        \n",
    "        # Убираем пробелы в начале и конце строк\n",
    "        text = '\\n'.join(line.strip() for line in text.split('\\n'))\n",
    "        \n",
    "        # Убираем пустые строки в начале и конце текста\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Заменяем множественные пробелы на одинарные внутри строк\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def _extract_text_from_docx(self, file_path: str) -> str:\n",
    "        \"\"\"Извлекает текст из DOCX файла\"\"\"\n",
    "        doc = DocxDocument(file_path)\n",
    "        return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "\n",
    "    def _extract_text_from_xlsx(self, file_path: str) -> str:\n",
    "        \"\"\"Извлекает текст из XLSX файла\"\"\"\n",
    "        df = pd.read_excel(file_path)\n",
    "        return df.to_string()\n",
    "\n",
    "    def _extract_text_from_pptx(self, file_path: str) -> str:\n",
    "        \"\"\"Извлекает текст из PPTX файла\"\"\"\n",
    "        prs = Presentation(file_path)\n",
    "        text = []\n",
    "        for slide in prs.slides:\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\"):\n",
    "                    text.append(shape.text)\n",
    "        return \"\\n\".join(text)\n",
    "\n",
    "    # def _extract_text_from_pdf(self, file_path: str) -> str:\n",
    "    #     \"\"\"Извлекает текст из PDF файла\"\"\"\n",
    "    #     text = []\n",
    "    #     with open(file_path, 'rb') as file:\n",
    "    #         pdf_reader = pypdf.PdfReader(file)\n",
    "    #         for page in pdf_reader.pages:\n",
    "    #             extracted_text = page.extract_text()\n",
    "    #             if extracted_text:\n",
    "    #                 text.append(extracted_text)\n",
    "    #                 # text.append(f\"Page {page+1}: {extracted_text}\") # ****\n",
    "    #     return \"\\n\".join(text)\n",
    "\n",
    "    def _extract_text_from_pdf(self, file_path: str) -> str:\n",
    "        \"\"\"Extracts text from PDF file, using OCR for image-based pages if necessary\"\"\"\n",
    "        text = []\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = pypdf.PdfReader(file)\n",
    "                for page_num, page in enumerate(pdf_reader.pages):\n",
    "                    # Attempt text extraction\n",
    "                    extracted_text = page.extract_text()\n",
    "                    \n",
    "                    # If no text found, use OCR\n",
    "                    if not extracted_text:\n",
    "                        images = convert_from_path(file_path, first_page=page_num + 1, last_page=page_num + 1)\n",
    "                        for image in images:\n",
    "                            extracted_text = pytesseract.image_to_string(image)\n",
    "                    \n",
    "                    # Add page label if you want to track pages in the output\n",
    "                    # if extracted_text:\n",
    "                    #     text.append(f\"Page {page_num + 1}:\\n{extracted_text}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting text from PDF {file_path}: {str(e)}\")\n",
    "        \n",
    "        return \"\\n\".join(text)\n",
    "\n",
    "    def _extract_text_from_csv(self, file_path: str) -> str:\n",
    "        \"\"\"Извлекает текст из CSV файла\"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df.to_string()\n",
    "\n",
    "    def _read_text_file(self, file_path: str) -> str:\n",
    "        \"\"\"Читает текст из MD файла\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "\n",
    "    def _read_json_file(self, file_path: str) -> str:\n",
    "        \"\"\"Читает текст из JSON файла\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            return json.dumps(data, ensure_ascii=False, indent=2)\n",
    "\n",
    "    @lru_cache(maxsize=1000)\n",
    "    def _load_document_cached(self, file_path: str, path_hash: str) -> str:\n",
    "        \"\"\"Кэшированная загрузка документа с поддержкой различных форматов\"\"\"\n",
    "        logger.debug(f\"Loading document: {file_path}\")\n",
    "        try:\n",
    "            extension = Path(file_path).suffix.lower()\n",
    "            \n",
    "            # Проверяем кэш\n",
    "            cache_file = Path(self.config.cache_dir) / f\"{hashlib.sha256(file_path.encode()).hexdigest()}.txt\"\n",
    "            if cache_file.exists():\n",
    "                with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                    return f.read()\n",
    "            \n",
    "            # Извлекаем текст в зависимости от формата\n",
    "            if extension == '.docx':\n",
    "                text = self._extract_text_from_docx(file_path)\n",
    "            elif extension == '.xlsx':\n",
    "                text = self._extract_text_from_xlsx(file_path)\n",
    "            elif extension == '.pptx':\n",
    "                text = self._extract_text_from_pptx(file_path)\n",
    "            elif extension == '.pdf':\n",
    "                text = self._extract_text_from_pdf(file_path)\n",
    "            elif extension == '.csv':\n",
    "                text = self._extract_text_from_csv(file_path)\n",
    "            elif extension == '.md':\n",
    "                text = self._read_text_file(file_path)\n",
    "            elif extension == '.json':\n",
    "                text = self._read_json_file(file_path)\n",
    "            else:\n",
    "                logger.warning(f\"Unsupported file extension: {extension}\")\n",
    "                return \"\"\n",
    "            \n",
    "            # Сохраняем в кэш\n",
    "            with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(text)\n",
    "                \n",
    "            return text\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {file_path}: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def create_documents(self, path: str) -> List[Document]:\n",
    "        \"\"\"Создает Langchain документы с метаданными из всех файлов в директории\"\"\"\n",
    "        path_hash = self._calculate_path_hash(path)\n",
    "        file_paths = self._get_all_files(path)\n",
    "        logger.info(f\"Found {len(file_paths)} supported files in {path}\")\n",
    "        \n",
    "        documents = []\n",
    "        for file_path in file_paths:\n",
    "            # Загружаем текст\n",
    "            text = self._load_document_cached(file_path, path_hash)\n",
    "            \n",
    "            # Предварительная обработка\n",
    "            text = self.preprocess_text(text)\n",
    "            \n",
    "            if not text:\n",
    "                continue\n",
    "            \n",
    "            # Получаем метаданные\n",
    "            metadata = {\n",
    "                \"source\": file_path,\n",
    "                \"filename\": Path(file_path).name,\n",
    "                \"extension\": Path(file_path).suffix.lower(),\n",
    "                \"relative_path\": os.path.relpath(file_path, path),\n",
    "                \"created_date\": datetime.fromtimestamp(os.path.getctime(file_path)).isoformat(),\n",
    "                \"modified_date\": datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat()\n",
    "            }\n",
    "            \n",
    "            # Разбиваем на чанки\n",
    "            texts = self.text_splitter.split_text(text)\n",
    "            \n",
    "            # Обрабатываем чанки\n",
    "            processed_chunks = []\n",
    "            for chunk in texts:\n",
    "                chunk = chunk.strip()\n",
    "                if chunk and len(chunk) <= self.config.chunk_size:\n",
    "                    processed_chunks.append(chunk)\n",
    "            \n",
    "            # Создаем документы\n",
    "            docs = [Document(page_content=chunk, metadata=metadata) for chunk in processed_chunks]\n",
    "            documents.extend(docs)\n",
    "            \n",
    "            logger.debug(f\"Created {len(docs)} chunks from {file_path}\")\n",
    "            \n",
    "        return documents\n",
    "\n",
    "    def process_documents(self, path: str) -> List[Document]:\n",
    "        \"\"\"Обрабатывает все документы в директории\"\"\"\n",
    "        logger.info(f\"Processing documents in {path}\")\n",
    "        return self.create_documents(path)\n",
    "\n",
    "def analyze_chunks(documents: List[Document]):\n",
    "    \"\"\"Анализирует полученные чанки\"\"\"\n",
    "    total_chunks = len(documents)\n",
    "    total_chars = sum(len(doc.page_content) for doc in documents)\n",
    "    avg_chunk_size = total_chars / total_chunks if total_chunks > 0 else 0\n",
    "    \n",
    "    print(f\"\\nAnalysis Results:\")\n",
    "    print(f\"Total chunks: {total_chunks}\")\n",
    "    print(f\"Average chunk size: {avg_chunk_size:.2f} characters\")\n",
    "    \n",
    "    print(\"\\nSample chunks:\")\n",
    "    for i, doc in enumerate(documents[:3]):\n",
    "        # print(f\"\\nChunk {i+1}:\")\n",
    "        print(f\"Length: {len(doc.page_content)}\")\n",
    "        print(f\"Source: {doc.metadata['source']}\")\n",
    "        print(f\"Content:\\n{doc.page_content}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing documents in ../data/TRAIN/\n",
      "INFO:__main__:Found 70 supported files in ../data/TRAIN/\n",
      "INFO:__main__:Using OCR for page 1 in ../data/TRAIN/0.pdf\n",
      "ERROR:__main__:Error performing OCR: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "INFO:__main__:Using OCR for page 5 in ../data/TRAIN/0.pdf\n",
      "ERROR:__main__:Error performing OCR: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "INFO:__main__:Using OCR for page 8 in ../data/TRAIN/0.pdf\n",
      "ERROR:__main__:Error performing OCR: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "INFO:__main__:Using OCR for page 10 in ../data/TRAIN/0.pdf\n",
      "ERROR:__main__:Error performing OCR: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "INFO:__main__:Using OCR for page 12 in ../data/TRAIN/0.pdf\n",
      "ERROR:__main__:Error performing OCR: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "INFO:__main__:Using OCR for page 17 in ../data/TRAIN/0.pdf\n",
      "ERROR:__main__:Error performing OCR: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 20 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 32 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 38 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 127 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 129 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 131 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 162 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 164 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 180 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 182 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 191 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 193 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 204 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 206 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 217 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 219 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 230 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 232 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 243 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 245 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 258 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 260 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 271 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 273 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 284 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 286 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 297 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 299 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 310 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 312 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 331 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 333 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 339 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 341 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 353 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 355 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 365 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 367 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 432 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 434 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 465 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 467 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 469 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 475 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 481 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 483 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 539 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 547 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 549 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 551 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 595 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 605 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 611 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 615 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 619 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 621 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 625 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 630 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 632 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 750 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 753 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 755 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 758 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 760 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 763 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 765 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 768 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 770 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 772 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 774 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 820 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 822 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 824 0 (offset 0)\n",
      "WARNING:pypdf._reader:Ignoring wrong pointing object 826 0 (offset 0)\n",
      "INFO:__main__:Using OCR for page 1 in ../data/TRAIN/10.pdf\n",
      "ERROR:__main__:Error performing OCR: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "INFO:__main__:Using OCR for page 2 in ../data/TRAIN/10.pdf\n",
      "ERROR:__main__:Error performing OCR: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "INFO:__main__:Using OCR for page 3 in ../data/TRAIN/10.pdf\n",
      "ERROR:__main__:Error performing OCR: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "INFO:__main__:Using OCR for page 4 in ../data/TRAIN/10.pdf\n",
      "ERROR:__main__:Error performing OCR: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "INFO:__main__:Using OCR for page 5 in ../data/TRAIN/10.pdf\n",
      "ERROR:__main__:Error performing OCR: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "INFO:__main__:Using OCR for page 6 in ../data/TRAIN/10.pdf\n",
      "ERROR:__main__:Error performing OCR: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "INFO:__main__:Using OCR for page 7 in ../data/TRAIN/10.pdf\n",
      "ERROR:__main__:Error performing OCR: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "INFO:__main__:Using OCR for page 8 in ../data/TRAIN/10.pdf\n",
      "ERROR:__main__:Error performing OCR: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "INFO:__main__:Using OCR for page 9 in ../data/TRAIN/10.pdf\n",
      "ERROR:__main__:Error performing OCR: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "INFO:__main__:Using OCR for page 10 in ../data/TRAIN/10.pdf\n",
      "ERROR:__main__:Error performing OCR: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "INFO:__main__:Using OCR for page 11 in ../data/TRAIN/10.pdf\n",
      "ERROR:__main__:Error performing OCR: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "INFO:__main__:Using OCR for page 12 in ../data/TRAIN/10.pdf\n",
      "ERROR:__main__:Error performing OCR: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "INFO:__main__:Using OCR for page 13 in ../data/TRAIN/10.pdf\n",
      "ERROR:__main__:Error performing OCR: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "INFO:__main__:Using OCR for page 14 in ../data/TRAIN/10.pdf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m loader \u001b[38;5;241m=\u001b[39m DocumentLoader(config)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Обрабатываем документы\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/TRAIN/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Анализируем результаты\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# analyze_chunks(documents)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 205\u001b[0m, in \u001b[0;36mDocumentLoader.process_documents\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Process all documents in directory\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing documents in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 166\u001b[0m, in \u001b[0;36mDocumentLoader.create_documents\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    163\u001b[0m documents \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m file_paths:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Load text\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_document_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Preprocess\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_text(text)\n",
      "Cell \u001b[0;32mIn[1], line 146\u001b[0m, in \u001b[0;36mDocumentLoader._load_document_cached\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Extract text\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_text_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Save to cache\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cache_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[1], line 119\u001b[0m, in \u001b[0;36mDocumentLoader._extract_text_from_pdf\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_ocr \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmin_text_length:\n\u001b[1;32m    118\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing OCR for page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m images:\n\u001b[1;32m    121\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_perform_ocr_on_page(images[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/mambaforge/envs/nlp/lib/python3.11/site-packages/pdf2image/pdf2image.py:251\u001b[0m, in \u001b[0;36mconvert_from_path\u001b[0;34m(pdf_path, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, ownerpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo, timeout, hide_annotations)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m uid, proc \u001b[38;5;129;01min\u001b[39;00m processes:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m         data, err \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired:\n\u001b[1;32m    253\u001b[0m         proc\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/mambaforge/envs/nlp/lib/python3.11/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/nlp/lib/python3.11/subprocess.py:2115\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2109\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2110\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2113\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2115\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/nlp/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Создаем конфигурацию с настройками по умолчанию\n",
    "config = Config()\n",
    "# Инициализируем загрузчик\n",
    "loader = DocumentLoader(config)\n",
    "# Обрабатываем документы\n",
    "documents = loader.process_documents(\"../data/TRAIN/\")\n",
    "\n",
    "# Анализируем результаты\n",
    "analyze_chunks(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17469\n",
      "id: 8716\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This, combined with a vibr ant deep-t ech ecosy st em and a sur g e in quantum startups, cr e at es a f ertil e gr ound f or inno v ation. The r obust ecosy st em of 125000 pl us startups and wide net work of MSMEs acr oss the nation will not onl y driv e domestic demand, e xport s and economic gr ow th but also position India as a l e ader in tack ling chall eng es acr oss div erse ind ustries who ar e g e aring up f or Quantum Computing now and soon f or web 3.0.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(len(documents))\n",
    "id = random.randint(0, len(documents))\n",
    "print('id:',id)\n",
    "print('-'*100)\n",
    "print(documents[id].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '../data/TRAIN/44.pdf',\n",
       " 'filename': '44.pdf',\n",
       " 'extension': '.pdf',\n",
       " 'relative_path': '44.pdf',\n",
       " 'created_date': '2024-11-08T19:37:26.704175',\n",
       " 'modified_date': '2024-07-24T16:02:18'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[id].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test Chuncks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# данные для теститрования: Вопрос, Фрагмент релевантного чанка (подстрока)\n",
    "tests = [\n",
    "    {\n",
    "        \"question\": \"Сколько человек пользуются интернетом?\",\n",
    "        \"answer\": \"По итогам 2023 года 5.3 млрд человек или 65,7% мирового населения пользовались интернетом\",\n",
    "        \"filename\": 1,\n",
    "        \"slide_number\": 2\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Какая ежедневная аудитория Рунета\",\n",
    "        \"answer\": \"Ежедневная аудитория Рунета достигла 95,3 млн человек, что составляет 66,4% от всего населения России\",\n",
    "        \"filename\": 1,\n",
    "        \"slide_number\": 2\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Какая доля проникновения социальных сетей\",\n",
    "        \"answer\": \"Проникновение социальных сетей в мире уже достигло 90%\",\n",
    "        \"filename\": 1,\n",
    "        \"slide_number\": 2\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Сколько интернет пользователей смотрят видеоролики\",\n",
    "        \"answer\": \"Видеоролики остаются крайне популярным форматом, практически все интернет-пользователи (92,3%) смотрят их на различных ресурсах\",\n",
    "        \"filename\": 1,\n",
    "        \"slide_number\": 3\n",
    "    },  \n",
    "    {\n",
    "        \"question\": \"Какая самая крупная социальная сеть в мире\",\n",
    "        \"answer\": \"Facebook с аудиторией 3030 млн\",\n",
    "        \"filename\": 1,\n",
    "        \"slide_number\": 5\n",
    "    },  \n",
    "    {\n",
    "        \"question\": \"Сколько минут в день проводят пользователей в социальных сетей\",\n",
    "        \"answer\": \"Среднесуточное использование социальных сетей интернет-пользователями во всем мире достигло 151 минуты в день в 2023 году\",\n",
    "        \"filename\": 1,\n",
    "        \"slide_number\": 6\n",
    "    },  \n",
    "    {\n",
    "        \"question\": \"В какой стране больше всего используют социальные сети в качестве источника новостей\",\n",
    "        \"answer\": \"Нигерия, 78%\",\n",
    "        \"filename\": 1,\n",
    "        \"slide_number\": 6\n",
    "    }, \n",
    "    {\n",
    "        \"question\": \"Какие основные преимущества использования социальных сетей\",\n",
    "        \"answer\": \"Уровень вовлечения 86%\",\n",
    "        \"filename\": 1,\n",
    "        \"slide_number\": 7\n",
    "    }, \n",
    "    {\n",
    "        \"question\": \"Ежемесячная аудитория TikTok по странам\",\n",
    "        \"answer\": \"Уровень вовлечения 86%\",\n",
    "        \"filename\": 1,\n",
    "        \"slide_number\": 7\n",
    "    }, \n",
    "        {\n",
    "        \"question\": \"What is the overall trend in playtime for gamers in 2024?\",\n",
    "        \"answer\": \"Playtime has shrunk significantly, with a decrease of 26% since its peak in Q1 2021. This trend is expected to continue into 2024 as players engage less frequently with games\",\n",
    "        \"filename\": 3,\n",
    "        \"slide_number\": 4\n",
    "    }, \n",
    "    {\n",
    "        \"question\": \"How did the PC and console gaming market perform in 2023?\",\n",
    "        \"answer\": \"The PC and console gaming market generated $93.5 billion in 2023, marking a modest growth of 2.6% year-over-year. However, the overall market is slowing down compared to previous years\",\n",
    "        \"filename\": 3,\n",
    "        \"slide_number\": 8\n",
    "    }, \n",
    "    {\n",
    "        \"question\": \"Which types of games are capturing most of the playtime?\",\n",
    "        \"answer\": \"Games that have been available for six or more years are capturing over half of the total playtime, indicating a trend where older titles dominate player engagement\",\n",
    "        \"filename\": 3,\n",
    "        \"slide_number\": 7\n",
    "    }, \n",
    "    {\n",
    "        \"question\": \"What are the main revenue sources for PC and console gaming?\",\n",
    "        \"answer\": \"In 2023, premium transactions accounted for 56% of PC spending and 57% of console spending, highlighting the importance of full-price game sales alongside in-game purchases\",\n",
    "        \"filename\": 3,\n",
    "        \"slide_number\": 9\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the forecast for player growth in the PC and console market from 2023 to 2026?\",\n",
    "        \"answer\": \"The forecast anticipates a compound annual growth rate (CAGR) of 1.6% for PC players and 3% for console players, indicating a slowdown in new player acquisition compared to previous years\",\n",
    "        \"filename\": 3,\n",
    "        \"slide_number\": 11\n",
    "    }, \n",
    "    {\n",
    "        \"question\": \"Какой онлайн кинотеатр самый популярный в России\",\n",
    "        \"answer\": \"Иви\",\n",
    "        \"filename\": 13,\n",
    "        \"slide_number\": 14\n",
    "    }, \n",
    "    {\n",
    "        \"question\": \"В какой индустрии самая дорогая цена за 1000 показов\",\n",
    "        \"answer\": \"Маркетинг 8.1€\",\n",
    "        \"filename\": 17,\n",
    "        \"slide_number\": 3\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"question\": \"Какой прогноз объема российской интернет-экономики на 2023 год\",\n",
    "        \"answer\": \"16.4 трлн руб.\",\n",
    "        \"filename\": 17,\n",
    "        \"slide_number\": 3\n",
    "    }, \n",
    "    {\n",
    "        \"question\": \"Сможет ли ИИ заменить человека?\",\n",
    "        \"answer\": \"Людей заменить ИИ не сможет (пока), но существенно изменит суть работы человека\",\n",
    "        \"filename\": 0,\n",
    "        \"slide_number\": 15\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Какова польза инфлюенс-маркетинга в сфере недвижимости?\",\n",
    "        \"answer\": \"В сфере недвижимости инфлюенс-маркетинг позволяет уникально и эффективно продвигать объекты, используя данные о потребительских предпочтениях, которые предоставляют инфлюенсеры\",\n",
    "        \"filename\": 4,\n",
    "        \"slide_number\": 30\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Why AI tools can be gamechangers for marketers?\",\n",
    "        \"answer\": \"AI tools can be gamechangers for marketers. They can expedite the brainstorming process with data-driven suggestions, optimize content for specific channels, and help automate content repurposing across various platforms.\",\n",
    "        \"filename\": 2,\n",
    "        \"slide_number\": 20\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Какая доля населения России использует отечественные социальные сети/онлайн-платформы?\",\n",
    "        \"answer\": \"70% населения России пользуются отечественными социальными сетями/онлайн-платформами\",\n",
    "        \"filename\": 6,\n",
    "        \"slide_number\": 10\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are big data revolutionizing in terms of companies?\",\n",
    "        \"answer\": \"Big data is revolutionizing how companies attain greater customer responsiveness and gain greater customer insights.\",\n",
    "        \"filename\": 8,\n",
    "        \"slide_number\": 36\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Что представляет собой AiData GPT?\",\n",
    "        \"answer\": \"AiData GPT - сегменты обезличенных идентификаторов пользователей, объединенные с помощью AI-технологии по определенным признакам\",\n",
    "        \"filename\": 10,\n",
    "        \"slide_number\": 15\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Что обеспечит больший интерес клиентов к спецпроекту?\",\n",
    "        \"answer\": \"Совмещение спецпроекта с распродажей обеспечит больший интерес клиентов\",\n",
    "        \"filename\": 12,\n",
    "        \"slide_number\": 24\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What brands will be successful?\",\n",
    "        \"answer\": \"Successful brands will be those which get clear on the role they have to play in the culture landscape, and the engagement levers needed to activate accordingly.\",\n",
    "        \"filename\": 16,\n",
    "        \"slide_number\": 10\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is social media company X planning to do to become a super app?\",\n",
    "        \"answer\": \"Social media company X is equally evolving its offerings by adding video, Gen AI and finance capabilities with the aim of becoming a super app\",\n",
    "        \"filename\": 18,\n",
    "        \"slide_number\": 11\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Какой результат замера по Weborama для видимости по OLV-размещениям?\",\n",
    "        \"answer\": \"82% средний показатель видимости по OLV-размещениям\",\n",
    "        \"filename\": 20,\n",
    "        \"slide_number\": 16\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Какая самая популярная площадка в России по объёму публикуемого контента\",\n",
    "        \"answer\": \"Telegram, Dau 50.9 млн пользователей\",\n",
    "        \"filename\": 27,\n",
    "        \"slide_number\": 3\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Какой самый главный тренд в видеоформатах\",\n",
    "        \"answer\": \"Главный тренд — рост востребованности видеоформатов, в первую очередь коротких вертикальных роликов. Почти половина опрошенных смотрят их чаще, чем раньше, — это формат контента, популярность которого только растёт.\",\n",
    "        \"filename\": 29,\n",
    "        \"slide_number\": 4\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Какой среднее время просмотра VK Видео на SmartTV\",\n",
    "        \"answer\": \"125 минут в день\",\n",
    "        \"filename\": 29,\n",
    "        \"slide_number\": 4\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: intfloat/multilingual-e5-large\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the model and device\n",
    "# model_name = 'deepvk/USER-bge-m3'                  # +++\n",
    "# model_name = 'DeepPavlov/rubert-base-cased'\n",
    "model_name = 'intfloat/multilingual-e5-large'\n",
    "# model_name = 'cointegrated/rubert-tiny2'\n",
    "# model_name = 'intfloat/e5-mistral-7b-instruct'\n",
    "# model_name = 'ai-forever/sbert_large_nlu_ru'            \n",
    "\n",
    "\n",
    "embeddings_dir = '../embeddings'\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(embeddings_dir, exist_ok=True)\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Save the model and tokenizer to the embeddings directory\n",
    "model.save_pretrained(embeddings_dir)\n",
    "tokenizer.save_pretrained(embeddings_dir)\n",
    "\n",
    "# Create embeddings using the saved model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: ../embeddings\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name ../embeddings. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "# Transform chunks into documents\n",
    "# documents = [Document(page_content=chunk.page_content if isinstance(chunk, Document) else chunk) for chunk in documents]\n",
    "\n",
    "# Create a vector store from documents\n",
    "db = FAISS.from_documents(documents, embeddings)\n",
    "# Save the FAISS database locally in the embeddings directory\n",
    "db.save_local(os.path.join(embeddings_dir, \"faiss_db\"))\n",
    "\n",
    "# Load the model and tokenizer from the embeddings directory\n",
    "loaded_model = AutoModel.from_pretrained(embeddings_dir)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(embeddings_dir)\n",
    "\n",
    "# Create embeddings again using the loaded model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embeddings_dir,\n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "\n",
    "# Load the FAISS database from the embeddings directory\n",
    "db = FAISS.load_local(\n",
    "    os.path.join(embeddings_dir, \"faiss_db\"), \n",
    "    embeddings, \n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve more documents with higher diversity\n",
    "# Useful if your dataset has many similar documents\n",
    "faiss_retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 5, 'lambda_mult': 0.25}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Randomly select one test case\n",
    "selected_test = random.choice(tests)\n",
    "# Assign the topic to text_input and the chunk to answer\n",
    "query = selected_test['question']\n",
    "answer = selected_test['answer']\n",
    "\n",
    "# Output the result\n",
    "print(Fore.GREEN ,f\"question: {query}\" , Fore.RESET)\n",
    "print(f\"answer: {answer}\")\n",
    "print('='*100)\n",
    "\n",
    "# Извлечение топ-3 кандидатов\n",
    "top_documents = faiss_retriever.invoke(query)\n",
    "# Печать топ-3 документов\n",
    "for idx, doc in enumerate(top_documents, start=1):\n",
    "    print(f\"Документ {idx}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlmrtnrt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmlmrtnrt\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mlmrtnrt' is not defined"
     ]
    }
   ],
   "source": [
    "mlmrtnrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Self Metricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import faiss\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def rag_faiss_evaluation(query: str, answer_model: List[str], model_name: str ,debug: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Выполняет поиск и оценку с учетом точного вхождения строки запроса.\n",
    "    \n",
    "    Параметры:\n",
    "    query (str): Исходный запрос\n",
    "    answer_model (List[str]): Список строк для поиска\n",
    "    debug (bool): Флаг для включения отладочной информации\n",
    "    \"\"\"\n",
    "    def normalize_text(text: str) -> str:\n",
    "        \"\"\"Нормализует текст для сравнения, удаляя лишние пробелы и приводя к нижнему регистру\"\"\"\n",
    "        text = text.replace('.',' .').replace(',',' ,')\n",
    "        return ' '.join(text.lower().split())\n",
    "\n",
    "    normalized_query = normalize_text(query)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\nНормализованный запрос: {normalized_query}\")\n",
    "\n",
    "    # Проверяем сначала точное вхождение и высокое совпадение слов\n",
    "    for idx, doc in enumerate(answer_model):\n",
    "        normalized_doc = normalize_text(doc)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\nДокумент {idx}:\")\n",
    "            print(f\"Нормализованный документ: {normalized_doc}\")\n",
    "            print(f\"Является подстрокой: {normalized_query in normalized_doc}\")\n",
    "            similarity = SequenceMatcher(None, normalized_query, normalized_doc).ratio()\n",
    "            print(f\"Процент совпадения: {similarity:.2f}\")\n",
    "\n",
    "        # Проверяем точное вхождение\n",
    "        if normalized_query in normalized_doc:\n",
    "            position_score = {0: 1.0, 1: 0.8, 2: 0.6, 3: 0.4, 4: 0.2}.get(idx, 0.0)\n",
    "            if debug:\n",
    "                print(f\"Найдено точное вхождение в документе {idx}\")\n",
    "                print(f\"Возвращаем score: {position_score}\")\n",
    "            return position_score\n",
    "\n",
    "        # Проверяем покрытие слов\n",
    "        words_query = set(normalized_query.split())\n",
    "        words_doc = set(normalized_doc.split())\n",
    "        common_words = words_query.intersection(words_doc)\n",
    "        coverage = len(common_words) / len(words_query)\n",
    "        \n",
    "        if coverage > 0.9:\n",
    "            position_score = {0: 1.0, 1: 0.8, 2: 0.6, 3: 0.4, 4: 0.2}.get(idx, 0.0)\n",
    "            if debug:\n",
    "                print(f\"Найдено высокое совпадение в документе {idx}\")\n",
    "                print(f\"Покрытие слов: {coverage:.2f}\")\n",
    "                print(f\"Возвращаем score: {position_score * coverage}\")\n",
    "            return position_score * coverage\n",
    "\n",
    "    # Если точного вхождения нет, используем семантический поиск\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    query_emb = model(**tokenizer(query, return_tensors=\"pt\")).pooler_output[0]\n",
    "    answer_emb = torch.stack([model(**tokenizer(a, return_tensors=\"pt\")).pooler_output[0] for a in answer_model])\n",
    "    \n",
    "    index = faiss.IndexFlatL2(answer_emb.shape[1])\n",
    "    index.add(answer_emb.detach().numpy())\n",
    "    \n",
    "    distances, indices = index.search(query_emb.detach().numpy().reshape(1, -1), 5)\n",
    "    \n",
    "    best_score = 0.0\n",
    "    for position, idx in enumerate(indices[0]):\n",
    "        text_similarity = SequenceMatcher(None, \n",
    "                                        normalize_text(query), \n",
    "                                        normalize_text(answer_model[idx])).ratio()\n",
    "        position_score = {0: 1.0, 1: 0.8, 2: 0.6, 3: 0.4, 4: 0.2}.get(position, 0.0)\n",
    "        score = position_score * text_similarity\n",
    "        best_score = max(best_score, score)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\nПозиция {position}, документ {idx}:\")\n",
    "            print(f\"Текстовое сходство: {text_similarity:.2f}\")\n",
    "            print(f\"Position score: {position_score:.2f}\")\n",
    "            print(f\"Итоговый score: {score:.2f}\")\n",
    "    \n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Тест: Embeddings, Similarity search\n",
    "https://huggingface.co/spaces/mteb/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список моделей для embeddings\n",
    "model_names=[\n",
    "             'intfloat/e5-mistral-7b-instruct',\n",
    "             'deepvk/USER-base',\n",
    "             'BAAI/bge-m3',\n",
    "             'intfloat/multilingual-e5-large',\n",
    "             'intfloat/multilingual-e5-large-instruct',\n",
    "             'deepvk/USER-bge-m3',\n",
    "             'intfloat/multilingual-e5-base',\n",
    "             'intfloat/multilingual-e5-small',\n",
    "             'ai-forever/ru-en-RoSBERTa',\n",
    "             'sergeyzh/LaBSE-ru-turbo',\n",
    "             'HIT-TMG/KaLM-embedding-multilingual-mini-v1',\n",
    "             'HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1',\n",
    "             'sergeyzh/rubert-tiny-turbo',\n",
    "             'cointegrated/rubert-tiny2',\n",
    "             'ai-forever/sbert_large_mt_nlu_ru',\n",
    "             'ai-forever/sbert_large_nlu_ru',\n",
    "             'DeepPavlov/rubert-base-cased-sentence',\n",
    "             'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
    "             'symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli',\n",
    "            'ai-forever/ruRoberta-large',\n",
    "            'ai-forever/ruT5-large',\n",
    "            'ai-forever/mGPT',\n",
    "            'ai-forever/FRED-T5-large-spell'\n",
    "             ]\n",
    "\n",
    "# model_kwargs = {'device': 'cpu'}\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "\n",
    "# Список для хранения результатов\n",
    "results = []\n",
    "\n",
    "# Итерация по моделям\n",
    "for model_name in model_names:\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "\n",
    "    # Создание embeddings с использованием сохраненной модели\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs\n",
    "    )\n",
    "    # Create a vector store from documents\n",
    "    db = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "    # Получаем retriever с параметрами MMR\n",
    "    faiss_retriever = db.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={'k': 5, 'lambda_mult': 0.25}\n",
    "    )\n",
    "\n",
    "    # Список для хранения результатов по каждой модели\n",
    "    model_scores = []\n",
    "\n",
    "    # Итерация по тестам\n",
    "    for ind in range(len(tests)):\n",
    "        query = tests[ind]['topic']\n",
    "        right_answer = tests[ind]['chunk']\n",
    "\n",
    "        # Извлечение топ-5 кандидатов\n",
    "        top_documents = faiss_retriever.invoke(query)\n",
    "        answer_model = [\n",
    "            top_documents[0].page_content,\n",
    "            top_documents[1].page_content,\n",
    "            top_documents[2].page_content,\n",
    "            top_documents[3].page_content,\n",
    "            top_documents[4].page_content\n",
    "        ]\n",
    "\n",
    "        # Вычисление оценки\n",
    "        score = rag_faiss_evaluation(right_answer, answer_model, model_name)\n",
    "        model_scores.append(score)\n",
    "\n",
    "    # Сохранение результатов по модели\n",
    "    results.append({\n",
    "        'model': model_name,\n",
    "        'average_score': sum(model_scores) / len(model_scores)  # Средняя оценка по всем тестам\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание датафрейма с результатами\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Сортировка моделей по средней оценке в убывающем порядке\n",
    "df_results_sorted = df_results.sort_values(by='average_score', ascending=False)\n",
    "\n",
    "# Сохранение в файл\n",
    "df_results_sorted.to_csv('model_scores.csv', index=False)\n",
    "\n",
    "# Вывод топ-3 лучших моделей\n",
    "top_models = df_results_sorted\n",
    "print(\"Top models based on average score:\")\n",
    "top_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BM25 retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "bm25_retriever.k = 5  # Retrieve top results\n",
    "\n",
    "# Список для хранения результатов\n",
    "results = []\n",
    "\n",
    "for ind in range(len(tests)):\n",
    "    query = tests[ind]['topic']\n",
    "    right_answer = tests[ind]['chunk']\n",
    "\n",
    "    # Извлечение топ-5 кандидатов\n",
    "    top_documents = bm25_retriever.invoke(query)\n",
    "    answer_model = [\n",
    "        top_documents[0].page_content,\n",
    "        top_documents[1].page_content,\n",
    "        top_documents[2].page_content,\n",
    "        top_documents[3].page_content,\n",
    "        top_documents[4].page_content\n",
    "    ]\n",
    "\n",
    "    # Вычисление оценки\n",
    "    score = rag_faiss_evaluation(right_answer, answer_model)\n",
    "    model_scores.append(score)\n",
    "\n",
    "# Сохранение результатов по модели\n",
    "results.append({\n",
    "    'model': 'bm25_retriever',\n",
    "    'average_score': sum(model_scores) / len(model_scores)  # Средняя оценка по всем тестам\n",
    "})\n",
    "\n",
    "# Создание датафрейма с результатами\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Сортировка моделей по средней оценке в убывающем порядке\n",
    "df_results_sorted = df_results.sort_values(by='average_score', ascending=False)\n",
    "\n",
    "# Сохранение в файл\n",
    "df_results_sorted.to_csv('model_scores.csv', index=False)\n",
    "\n",
    "# Вывод топ-3 лучших моделей\n",
    "top_models = df_results_sorted\n",
    "print(\"Top models based on average score:\")\n",
    "top_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Implementing Hybrid Search with ensemble Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'intfloat/multilingual-e5-large'\n",
    "model_kwargs = {'device': 'gpu'}\n",
    "# Создание embeddings с использованием сохраненной модели\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "# Create a vector store from documents\n",
    "db = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Получаем retriever с параметрами MMR\n",
    "faiss_retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 5, 'lambda_mult': 0.25}\n",
    ")\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[faiss_retriever,\n",
    "                                                   bm25_retriever], weights=[0.6, 0.4 ])\n",
    "\n",
    "# Список для хранения результатов\n",
    "results = []\n",
    "\n",
    "for ind in range(len(tests)):\n",
    "    query = tests[ind]['topic']\n",
    "    right_answer = tests[ind]['chunk']\n",
    "\n",
    "    # Извлечение топ-5 кандидатов\n",
    "    top_documents = ensemble_retriever.invoke(query)\n",
    "    answer_model = [\n",
    "        top_documents[0].page_content,\n",
    "        top_documents[1].page_content,\n",
    "        top_documents[2].page_content,\n",
    "        top_documents[3].page_content,\n",
    "        top_documents[4].page_content\n",
    "    ]\n",
    "\n",
    "    # Вычисление оценки\n",
    "    score = rag_faiss_evaluation(right_answer, answer_model)\n",
    "    model_scores.append(score)\n",
    "\n",
    "# Сохранение результатов по модели\n",
    "results.append({\n",
    "    'model': 'EnsembleRetriever',\n",
    "    'average_score': sum(model_scores) / len(model_scores)  # Средняя оценка по всем тестам\n",
    "})\n",
    "\n",
    "# Создание датафрейма с результатами\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Сортировка моделей по средней оценке в убывающем порядке\n",
    "df_results_sorted = df_results.sort_values(by='average_score', ascending=False)\n",
    "\n",
    "# Сохранение в файл\n",
    "df_results_sorted.to_csv('model_scores.csv', index=False)\n",
    "\n",
    "# Вывод топ-3 лучших моделей\n",
    "top_models = df_results_sorted\n",
    "print(\"Top models based on average score:\")\n",
    "top_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ReRanker\n",
    "https://huggingface.co/amberoad/bert-multilingual-passage-reranking-msmarco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('amberoad/bert-multilingual-passage-reranking-msmarco',device='gpu')\n",
    "\n",
    "# Список для хранения результатов\n",
    "results = []\n",
    "\n",
    "for ind in range(len(tests)):\n",
    "    query = tests[ind]['topic']\n",
    "    right_answer = tests[ind]['chunk']\n",
    "\n",
    "    # Векторизация запроса\n",
    "    query_embedding = model.encode(query)\n",
    "    # Векторизация документов\n",
    "    doc_embeddings = model.encode([doc.page_content for doc in documents])\n",
    "    # Вычисление косинусного сходства\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, doc_embeddings)\n",
    "    # Преобразование в список\n",
    "    cosine_scores = cosine_scores[0].tolist()\n",
    "    # Объединение документов и их оценок\n",
    "    docs_with_scores = list(zip(documents, cosine_scores))\n",
    "    # Сортировка по оценкам\n",
    "    # Sort by the last float in each tuple (descending)\n",
    "    top_documents = sorted(docs_with_scores, key=lambda doc: doc[1], reverse=True)\n",
    "    answer_model = [\n",
    "        top_documents[0][0].page_content,\n",
    "        top_documents[1][0].page_content,\n",
    "        top_documents[2][0].page_content,\n",
    "        top_documents[3][0].page_content,\n",
    "        top_documents[4][0].page_content\n",
    "    ]\n",
    "\n",
    "    # Вычисление оценки\n",
    "    score = rag_faiss_evaluation(right_answer, answer_model)\n",
    "    model_scores.append(score)\n",
    "\n",
    "# Сохранение результатов по модели\n",
    "results.append({\n",
    "    'model': 'reranking-msmarco',\n",
    "    'average_score': sum(model_scores) / len(model_scores)  # Средняя оценка по всем тестам\n",
    "})\n",
    "\n",
    "# Создание датафрейма с результатами\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Сортировка моделей по средней оценке в убывающем порядке\n",
    "df_results_sorted = df_results.sort_values(by='average_score', ascending=False)\n",
    "\n",
    "# Сохранение в файл\n",
    "df_results_sorted.to_csv('model_scores.csv', index=False)\n",
    "\n",
    "# Вывод топ-3 лучших моделей\n",
    "top_models = df_results_sorted\n",
    "print(\"Top models based on average score:\")\n",
    "top_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# CrossEncoder\n",
    "https://huggingface.co/spaces/mteb/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Optional\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "\n",
    "# cross_encoder = CrossEncoder('intfloat/e5-mistral-7b-instruct',device=\"cpu\")\n",
    "cross_encoder = CrossEncoder('BAAI/bge-m3',device=\"cpu\")\n",
    "\n",
    "def rerank_documents(query: str, \n",
    "                     documents: List[Document], \n",
    "                     cross_encoder: CrossEncoder, \n",
    "                     top_k: int = 5) -> List[Tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Переранжирование документов с использованием CrossEncoder.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Поисковый запрос\n",
    "        documents (List[Document]): Список документов для ранжирования\n",
    "        cross_encoder (CrossEncoder): Модель CrossEncoder\n",
    "        top_k (int): Количество топовых документов для возврата\n",
    "        \n",
    "    Returns:\n",
    "        List[Tuple[Document, float]]: Список кортежей (документ, score) отсортированный по релевантности\n",
    "    \"\"\"\n",
    "    # Создаем пары запрос-документ для каждого документа\n",
    "    pairs = [[query, doc.page_content] for doc in documents]\n",
    "    \n",
    "    # Получаем scores от CrossEncoder\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Создаем список кортежей (документ, score)\n",
    "    doc_score_pairs = list(zip(documents, scores))\n",
    "    \n",
    "    # Сортируем по score по убыванию\n",
    "    ranked_docs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Возвращаем top_k документов\n",
    "    return ranked_docs[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список для хранения результатов\n",
    "results = []\n",
    "\n",
    "for ind in range(len(tests)):\n",
    "    query = tests[ind]['topic']\n",
    "    right_answer = tests[ind]['chunk']\n",
    "\n",
    "    ranked_results = rerank_documents(query, documents, cross_encoder)\n",
    "    answer_model = [\n",
    "        top_documents[0][0].page_content,\n",
    "        top_documents[1][0].page_content,\n",
    "        top_documents[2][0].page_content,\n",
    "        top_documents[3][0].page_content,\n",
    "        top_documents[4][0].page_content\n",
    "    ]\n",
    "\n",
    "    # Вычисление оценки\n",
    "    score = rag_faiss_evaluation(right_answer, answer_model)\n",
    "    model_scores.append(score)\n",
    "\n",
    "# Сохранение результатов по модели\n",
    "results.append({\n",
    "    'model': 'reranking-msmarco',\n",
    "    'average_score': sum(model_scores) / len(model_scores)  # Средняя оценка по всем тестам\n",
    "})\n",
    "\n",
    "# Создание датафрейма с результатами\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Сортировка моделей по средней оценке в убывающем порядке\n",
    "df_results_sorted = df_results.sort_values(by='average_score', ascending=False)\n",
    "\n",
    "# Сохранение в файл\n",
    "df_results_sorted.to_csv('model_scores.csv', index=False)\n",
    "\n",
    "# Вывод топ-3 лучших моделей\n",
    "top_models = df_results_sorted\n",
    "print(\"Top models based on average score:\")\n",
    "top_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# HybridReranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from langchain.schema import Document\n",
    "\n",
    "class HybridReranker:\n",
    "    def __init__(\n",
    "        self,\n",
    "        bm25_retriever,\n",
    "        faiss_retriever,\n",
    "        model_name='sentence-transformers/all-MiniLM-L6-v2'\n",
    "    ):\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "        self.faiss_retriever = faiss_retriever\n",
    "        self.encoder = SentenceTransformer(model_name , device='cpu')\n",
    "    \n",
    "    def get_combined_documents(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int = 5,\n",
    "        bm25_weight: float = 0.4,\n",
    "        faiss_weight: float = 0.4,\n",
    "        diversity_weight: float = 0.2\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Получает и комбинирует результаты из обоих ретриверов с учетом разнообразия\n",
    "        \n",
    "        Args:\n",
    "            query: Пользовательский запрос\n",
    "            k: Количество документов для возврата\n",
    "            bm25_weight: Вес для BM25 скоров\n",
    "            faiss_weight: Вес для FAISS скоров\n",
    "            diversity_weight: Вес для разнообразия документов\n",
    "        \"\"\"\n",
    "        # Получаем документы из обоих ретриверов\n",
    "        bm25_docs = self.bm25_retriever.get_relevant_documents(query)\n",
    "        faiss_docs = self.faiss_retriever.get_relevant_documents(query)\n",
    "        \n",
    "        # Объединяем уникальные документы\n",
    "        all_docs = list({doc.page_content: doc for doc in bm25_docs + faiss_docs}.values())\n",
    "        \n",
    "        if not all_docs:\n",
    "            return []\n",
    "        \n",
    "        # Получаем эмбеддинги для запроса и документов\n",
    "        query_embedding = self.encoder.encode(query)\n",
    "        doc_embeddings = self.encoder.encode([doc.page_content for doc in all_docs])\n",
    "        \n",
    "        # Рассчитываем скоры\n",
    "        scores = self._calculate_scores(\n",
    "            query,\n",
    "            query_embedding,\n",
    "            all_docs,\n",
    "            doc_embeddings,\n",
    "            bm25_docs,\n",
    "            faiss_docs,\n",
    "            bm25_weight,\n",
    "            faiss_weight,\n",
    "            diversity_weight\n",
    "        )\n",
    "        \n",
    "        # Сортируем документы по финальному скору\n",
    "        ranked_pairs = sorted(\n",
    "            zip(all_docs, scores),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        return [doc for doc, _ in ranked_pairs[:k]]\n",
    "    \n",
    "    def _calculate_scores(\n",
    "        self,\n",
    "        query: str,\n",
    "        query_embedding: np.ndarray,\n",
    "        all_docs: List[Document],\n",
    "        doc_embeddings: np.ndarray,\n",
    "        bm25_docs: List[Document],\n",
    "        faiss_docs: List[Document],\n",
    "        bm25_weight: float,\n",
    "        faiss_weight: float,\n",
    "        diversity_weight: float\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Рассчитывает комбинированные скоры для документов\"\"\"\n",
    "        \n",
    "        # 1. BM25 скоры\n",
    "        bm25_scores = np.zeros(len(all_docs))\n",
    "        for i, doc in enumerate(all_docs):\n",
    "            # Если документ был в топе BM25, даем ему высокий скор\n",
    "            if doc in bm25_docs:\n",
    "                bm25_scores[i] = 1.0 - (bm25_docs.index(doc) / len(bm25_docs))\n",
    "        \n",
    "        # 2. FAISS скоры\n",
    "        faiss_scores = np.zeros(len(all_docs))\n",
    "        for i, doc in enumerate(all_docs):\n",
    "            if doc in faiss_docs:\n",
    "                faiss_scores[i] = 1.0 - (faiss_docs.index(doc) / len(faiss_docs))\n",
    "        \n",
    "        # 3. Разнообразие (diversity) через косинусное расстояние между документами\n",
    "        diversity_scores = np.zeros(len(all_docs))\n",
    "        for i, emb_i in enumerate(doc_embeddings):\n",
    "            # Среднее расстояние до других документов\n",
    "            distances = [\n",
    "                cosine(emb_i, emb_j) \n",
    "                for j, emb_j in enumerate(doc_embeddings) \n",
    "                if i != j\n",
    "            ]\n",
    "            if distances:\n",
    "                diversity_scores[i] = np.mean(distances)\n",
    "        \n",
    "        # Нормализация скоров\n",
    "        bm25_scores = bm25_scores / (np.max(bm25_scores) + 1e-8)\n",
    "        faiss_scores = faiss_scores / (np.max(faiss_scores) + 1e-8)\n",
    "        diversity_scores = diversity_scores / (np.max(diversity_scores) + 1e-8)\n",
    "        \n",
    "        # Комбинируем скоры с весами\n",
    "        final_scores = (\n",
    "            bm25_weight * bm25_scores +\n",
    "            faiss_weight * faiss_scores +\n",
    "            diversity_weight * diversity_scores\n",
    "        )\n",
    "        \n",
    "        return final_scores\n",
    "    \n",
    "    def analyze_ranking(\n",
    "        self,\n",
    "        query: str,\n",
    "        documents: List[Document],\n",
    "        top_k: int = 10\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Анализирует причины ранжирования для топ-K документов\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        query_embedding = self.encoder.encode(query)\n",
    "        doc_embeddings = self.encoder.encode([doc.page_content for doc in documents])\n",
    "        \n",
    "        for i, doc in enumerate(documents[:top_k]):\n",
    "            analysis = {\n",
    "                'content': doc.page_content[:200] + \"...\",\n",
    "                'in_bm25_top': doc in self.bm25_retriever.get_relevant_documents(query),\n",
    "                'in_faiss_top': doc in self.faiss_retriever.get_relevant_documents(query),\n",
    "                'semantic_similarity': 1 - cosine(query_embedding, doc_embeddings[i]),\n",
    "                'metadata': doc.metadata if hasattr(doc, 'metadata') else {}\n",
    "            }\n",
    "            results.append(analysis)\n",
    "            \n",
    "        return results\n",
    "\n",
    "# Пример использования:\n",
    "def demo_reranking(query: str, bm25_retriever, faiss_retriever):\n",
    "    # Создаем ранжировщик\n",
    "    reranker = HybridReranker(bm25_retriever, faiss_retriever)\n",
    "    \n",
    "    # Получаем переранжированные документы\n",
    "    reranked_docs = reranker.get_combined_documents(\n",
    "        query,\n",
    "        k=10,  # топ-3 документа\n",
    "        bm25_weight=0.4,\n",
    "        faiss_weight=0.4,\n",
    "        diversity_weight=0.2\n",
    "    )\n",
    "    \n",
    "    # Анализируем результаты\n",
    "    analysis = reranker.analyze_ranking(query, reranked_docs)\n",
    "    \n",
    "    return reranked_docs, analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'intfloat/multilingual-e5-large'\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "# Создание embeddings с использованием сохраненной модели\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "# Create a vector store from documents\n",
    "db = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Получаем retriever с параметрами MMR\n",
    "faiss_retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 5, 'lambda_mult': 0.25}\n",
    ")\n",
    "# Initialize the BM25 retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "bm25_retriever.k = 5  # Retrieve top results\n",
    "\n",
    "# Создаем ранжировщик\n",
    "reranker = HybridReranker(bm25_retriever, faiss_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    bm25_weight = trial.suggest_float('bm25_weight', 0.0, 1.0)\n",
    "    faiss_weight = trial.suggest_float('faiss_weight', 0.0, 1.0)\n",
    "    diversity_weight = 1 - bm25_weight - faiss_weight\n",
    "    \n",
    "    model_scores = []\n",
    "    for ind in range(len(tests)):\n",
    "        query = tests[ind]['topic']\n",
    "        right_answer = tests[ind]['chunk']\n",
    "        \n",
    "        reranked_docs = reranker.get_combined_documents(\n",
    "            query,\n",
    "            k=5,\n",
    "            bm25_weight=bm25_weight,\n",
    "            faiss_weight=faiss_weight,\n",
    "            diversity_weight=diversity_weight\n",
    "        )\n",
    "        answer_model = [\n",
    "            reranked_docs[0].page_content,\n",
    "            reranked_docs[0].page_content,\n",
    "            reranked_docs[0].page_content,\n",
    "            reranked_docs[0].page_content,\n",
    "            reranked_docs[0].page_content\n",
    "        ]\n",
    "        \n",
    "        score = rag_faiss_evaluation(right_answer, answer_model)\n",
    "        model_scores.append(score)\n",
    "    \n",
    "    average_score = sum(model_scores) / len(model_scores)\n",
    "    return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print('Optimal hyperparameters:')\n",
    "print(f'bm25_weight: {study.best_params[\"bm25_weight\"]}')\n",
    "print(f'faiss_weight: {study.best_params[\"faiss_weight\"]}')\n",
    "print(f'diversity_weight: {1 - study.best_params[\"bm25_weight\"] - study.best_params[\"faiss_weight\"]}')\n",
    "print(f'Best average score: {study.best_value}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
